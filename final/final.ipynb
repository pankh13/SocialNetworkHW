{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "# read a pos/neg dir\n",
    "# each document is a review\n",
    "corpus_folder = '/home/eric/Dropbox/490/final/review_polarity.v2/txt_sentoken/'\n",
    "def readDir(senti_folder, pattern):\n",
    "    file_list = []\n",
    "    path_pattern = os.path.join(corpus_folder, senti_folder, pattern + '*.txt')\n",
    "    all_txt_paths = glob.glob(path_pattern)\n",
    "    # !!!!! I am only taking the first top_doc_num dcouments in pos/neg folder\n",
    "    for file_path in all_txt_paths:\n",
    "        word_List = readFile(file_path)\n",
    "        file_list.append(word_List)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        tokenzied_words = f.read().split()\n",
    "        return tokenzied_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_file_list = readDir('pos', 'cv[0-8]')\n",
    "train_neg_file_list = readDir('neg', 'cv[0-8]')\n",
    "train_pos_labels = [1 for i in range(len(train_pos_file_list))]\n",
    "train_neg_labels = [0 for i in range(len(train_neg_file_list))]\n",
    "\n",
    "test_pos_file_list = readDir('pos', 'cv9')\n",
    "test_neg_file_list = readDir('neg', 'cv9')\n",
    "test_pos_labels = [1 for i in range(len(test_pos_file_list))]\n",
    "test_neg_labels = [0 for i in range(len(test_neg_file_list))]\n",
    "\n",
    "train_file_list = train_pos_file_list + train_neg_file_list\n",
    "test_file_list = test_pos_file_list + test_neg_file_list\n",
    "\n",
    "train_labels = train_pos_labels + train_neg_labels\n",
    "test_labels = test_pos_labels + test_neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tokenized):\n",
    "    punctuation_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "#     unique_punctuation_free = set(punctuation_free)\n",
    "    return ' '.join(punctuation_free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M1\n",
    "Unigrams (absence/presence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list_clean = [clean(doc) for doc in train_file_list]\n",
    "test_file_list_clean = [clean(doc) for doc in test_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "# HashingVectorizer is not allowing get_feature_names method, idk why though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english',binary=True)\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 37679)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 803.0 film\n",
      "1 668.0 movie\n",
      "1 653.0 like\n",
      "1 578.0 just\n",
      "1 576.0 time\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 772.0 film\n",
      "0 733.0 movie\n",
      "0 693.0 like\n",
      "0 612.0 just\n",
      "0 554.0 time\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def show_most_informative_features(vectorizer, classifier, n=5):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()  \n",
    "    topn_pos_class = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_neg_class = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]    \n",
    "\n",
    "    print(\"Important words in positive reviews\")\n",
    "    for coef, feature in topn_pos_class:\n",
    "        print(class_labels[1], coef, feature) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in negative reviews\")\n",
    "    for coef, feature in topn_neg_class:\n",
    "        print(class_labels[0], coef, feature)     \n",
    "        \n",
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M2\n",
    "Unigrams with frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english',binary=False)\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean])\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 4703.0 film\n",
      "1 2227.0 movie\n",
      "1 1636.0 like\n",
      "1 1200.0 just\n",
      "1 1130.0 story\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 3866.0 film\n",
      "0 2900.0 movie\n",
      "0 1690.0 like\n",
      "0 1392.0 just\n",
      "0 1043.0 time\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M3\n",
    "Unigrams (only adjectives/adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "def clean_adjv(tokenized):\n",
    "    punctuation_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "    word_posTags = pos_tag(punctuation_free, tagset='universal')\n",
    "    adjv_words = [t[0] for t in word_posTags if t[1] in (\"ADJ\",\"ADV\")]\n",
    "    return ' '.join(adjv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dilicious badly'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_adjv([\"I\",\"want\" ,\"this\" ,\"dilicious\" ,\"lunch\",'badly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list_clean_adjv = [clean_adjv(doc) for doc in train_file_list]\n",
    "test_file_list_clean_adjv = [clean_adjv(doc) for doc in test_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english',binary=False)\n",
    "#,\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean_adjv])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean_adjv])\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 1199.0 just\n",
      "1 1099.0 good\n",
      "1 729.0 best\n",
      "1 693.0 really\n",
      "1 691.0 little\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 1391.0 just\n",
      "0 1012.0 good\n",
      "0 928.0 bad\n",
      "0 709.0 really\n",
      "0 660.0 little\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M4\n",
    "Unigrams (sublinear tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.845\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', sublinear_tf=True)\n",
    "#,max_features=10000\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean])\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer(min_df = 5, stop_words='english', sublinear_tf=True, max_df=0.8)\n",
    "#,max_features=10000\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean])\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 22.321209797866082 movie\n",
      "1 18.722587051105048 like\n",
      "1 16.861012933794072 story\n",
      "1 16.81347161698053 life\n",
      "1 16.742999677712586 just\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 27.18739892096153 movie\n",
      "0 21.17460434117034 like\n",
      "0 19.76141374526718 just\n",
      "0 18.672792925114685 bad\n",
      "0 17.070786727241277 good\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M5\n",
    "Bigrams (absence/presence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(2,2),binary=True)\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 448758)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 108.0 special effects\n",
      "1 106.0 ve seen\n",
      "1 75.0 year old\n",
      "1 75.0 new york\n",
      "1 63.0 takes place\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 112.0 special effects\n",
      "0 98.0 ve seen\n",
      "0 71.0 new york\n",
      "0 68.0 year old\n",
      "0 68.0 looks like\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tokenized):\n",
    "    punctuation_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "#     unique_punctuation_free = set(punctuation_free)\n",
    "    return ' '.join(punctuation_free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list_clean = [clean(doc) for doc in train_file_list]\n",
    "test_file_list_clean = [clean(doc) for doc in test_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "# HashingVectorizer is not allowing get_feature_names method, idk why though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english',binary=True)\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 26894)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.865\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 847.0 thi\n",
      "1 820.0 film\n",
      "1 809.0 hi\n",
      "1 742.0 ha\n",
      "1 717.0 movi\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 860.0 thi\n",
      "0 799.0 film\n",
      "0 774.0 hi\n",
      "0 758.0 movi\n",
      "0 713.0 ha\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M2\n",
    "Unigrams with frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english',binary=False)\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean])\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 5533.0 film\n",
      "1 5020.0 hi\n",
      "1 4158.0 thi\n",
      "1 2660.0 movi\n",
      "1 2329.0 ha\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 4513.0 film\n",
      "0 4420.0 thi\n",
      "0 3594.0 hi\n",
      "0 3246.0 movi\n",
      "0 2199.0 wa\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M3\n",
    "Unigrams (only adjectives/adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "def clean_adjv(tokenized):\n",
    "    punctuation_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "    word_posTags = pos_tag(punctuation_free, tagset='universal')\n",
    "    adjv_words = [stemmer.stem(t[0]) for t in word_posTags if t[1] in (\"ADJ\",\"ADV\")]\n",
    "    return ' '.join(adjv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dilici badli'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_adjv([\"I\",\"want\" ,\"this\" ,\"dilicious\" ,\"lunch\",'badly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list_clean_adjv = [clean_adjv(doc) for doc in train_file_list]\n",
    "test_file_list_clean_adjv = [clean_adjv(doc) for doc in test_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english',binary=False)\n",
    "#,\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean_adjv])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean_adjv])\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 1199.0 just\n",
      "1 1100.0 good\n",
      "1 1025.0 veri\n",
      "1 1013.0 onli\n",
      "1 729.0 best\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 1391.0 just\n",
      "0 1203.0 onli\n",
      "0 1012.0 good\n",
      "0 928.0 bad\n",
      "0 709.0 realli\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M4\n",
    "Unigrams (sublinear tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer(min_df = 5, stop_words='english', sublinear_tf=True, max_df=0.8)\n",
    "#,max_features=10000\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean])\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 22.82210910508472 wa\n",
      "1 20.66562603602157 charact\n",
      "1 20.59120438981204 like\n",
      "1 19.767684246864327 make\n",
      "1 19.341010803351747 time\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 24.87719289173477 wa\n",
      "0 23.455621053910058 like\n",
      "0 21.271166445501652 charact\n",
      "0 20.9847037027419 just\n",
      "0 19.927320335592047 bad\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M5\n",
    "Bigrams (absence/presence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(2,2),binary=True)\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 426650)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 374.0 thi film\n",
      "1 229.0 thi movi\n",
      "1 108.0 special effect\n",
      "1 108.0 film wa\n",
      "1 107.0 hi wife\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 349.0 thi film\n",
      "0 291.0 thi movi\n",
      "0 145.0 look like\n",
      "0 124.0 like thi\n",
      "0 115.0 special effect\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lexicon = pd.read_csv(\"NRC_Emotion.txt\", sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon.columns = ['word','cat','flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_clean = lexicon[lexicon.cat.isin(['positive','negative'])].groupby(\"word\",as_index=False).mean()\n",
    "indicators = set(lexicon_clean[lexicon_clean.flag > 0][\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "lexicon_clean = lexicon[lexicon.word.isin(indicators)][lexicon.cat.isin(['positive','negative'])][lexicon.flag==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lexicon_clean.flag = (lexicon_clean['cat'] == \"positive\")*1 - (lexicon_clean['cat'] == \"negative\")*1\n",
    "lexicon_clean =lexicon_clean.groupby(\"word\",as_index=False).mean()\n",
    "lexicon_clean=lexicon_clean.reset_index()[['word','flag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {v: k for k, v in lexicon_clean['word'].to_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "def clean_lemma(tokenized):\n",
    "    punctuation_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "#     lemma_verb = [lemmatizer.lemmatize(word,'v') for word in punctuation_free]\n",
    "#     lemma_noun = [lemmatizer.lemmatize(word,'n') for word in lemma_verb]\n",
    "    word_posTags = pos_tag(punctuation_free, tagset='universal')\n",
    "    adjv_words = [t[0] for t in word_posTags if t[1] in (\"ADJ\",\"ADV\")]\n",
    "    return ' '.join(adjv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list_clean_lemma = [clean_lemma(doc) for doc in train_file_list]\n",
    "test_file_list_clean_lemma = [clean_lemma(doc) for doc in test_file_list]\n",
    "vectorizer = CountVectorizer(stop_words='english',binary=False , vocabulary=vocabulary)\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean_lemma])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean_lemma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "lexicon_value = [{k:v} for k,v in lexicon_clean['flag'].to_dict().items()]\n",
    "v = DictVectorizer(sparse=False)\n",
    "X = v.fit_transform(lexicon_value)\n",
    "\n",
    "threshold = test_features.dot(X).sum(axis=1).mean()\n",
    "predictions = (test_features.dot(X).sum(axis=1) > threshold) * 1\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "def clean_adjv(tokenized):\n",
    "    punctuation_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "    word_posTags = pos_tag(punctuation_free, tagset='universal')\n",
    "    adjv_words = [stemmer.stem(t[0]) for t in word_posTags if t[1] in (\"ADJ\",\"ADV\")]\n",
    "    return ' '.join(adjv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list_clean_adjv = [clean_adjv(doc) for doc in train_file_list]\n",
    "test_file_list_clean_adjv = [clean_adjv(doc) for doc in test_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english',binary=False)\n",
    "#,\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean_adjv])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean_adjv])\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 5, stop_words='english', sublinear_tf=True, max_df=0.8)\n",
    "#,max_features=10000\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean_adjv])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean_adjv])\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "#,max_features=10000\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean_adjv])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean_adjv])\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for this part I'll try using fasttext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "# read a pos/neg dir\n",
    "# each document is a review\n",
    "corpus_folder = '/home/eric/Dropbox/490/final/review_polarity.v2/txt_sentoken/'\n",
    "def readDir(senti_folder, pattern, label):\n",
    "    file_list = []\n",
    "    path_pattern = os.path.join(corpus_folder, senti_folder, pattern + '*.txt')\n",
    "    all_txt_paths = glob.glob(path_pattern)\n",
    "    # !!!!! I am only taking the first top_doc_num dcouments in pos/neg folder\n",
    "    for file_path in all_txt_paths:\n",
    "        doc = readFile(file_path)\n",
    "        title = file_path.split(\"_\")[-1].split(\".\")[0]\n",
    "        file_list.append(label + \" \" + title + \": \" + doc)\n",
    "    return file_list\n",
    "\n",
    "def readFile(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        tokenzied_words = \" \".join(f.read().split('\\n'))\n",
    "        return tokenzied_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fasttext_pos = readDir('pos', 'cv[0-8]','__label__1')\n",
    "train_fasttext_neg = readDir('neg', 'cv[0-8]','__label__2')\n",
    "\n",
    "test_fasttext_pos = readDir('pos', 'cv9','__label__1')\n",
    "test_fasttext_neg = readDir('neg', 'cv9','__label__2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input_fasttext.txt\",'w') as f:\n",
    "    f.write(\"\\n\".join(train_fasttext_pos + train_fasttext_neg))\n",
    "    \n",
    "with open(\"test_fasttext.txt\",'w') as f:\n",
    "    f.write(\"\\n\".join(test_fasttext_pos + test_fasttext_neg))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "then the following command was ran:\n",
    "\n",
    "./fasttext supervised -input /home/eric/Dropbox/490/final/input_fasttext.txt -output /home/eric/Dropbox/490/final/model_fasttext -label __label__ -epoch 100 -lr 0.5 -wordNgrams 1\n",
    "\n",
    "./fasttext test /home/eric/Dropbox/490/final/model_fasttext.bin /home/eric/Dropbox/490/final/test_fasttext.txt 1\n",
    "\n",
    "after tuning the parameters, we had a best result of 0.88 at the end. (ngram to be 1 or 2 doesn't matter so much, but ngram >=3 will get the performance far worse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
